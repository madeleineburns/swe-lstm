{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40008168-e76c-4f8f-a134-a431d7218247",
   "metadata": {},
   "source": [
    "# Data Management\n",
    "This notebook includes code for managing data, including generating representative sets of testing sites, running models on these testing sites, and evaluating the qualities/quantities of training and testing sets. The code is not meant to be run together; each section should be run independently when required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f53fe814-333b-4d9b-9470-474f52d09e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preliminaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random as rn\n",
    "import pickle\n",
    "import traceback\n",
    "import torch\n",
    "import hf_hydrodata as hf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from _lstm import *\n",
    "from _data import *\n",
    "\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "trap = io.StringIO()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5511002-5f94-4716-8307-88ec6465041c",
   "metadata": {},
   "source": [
    "# Numbers of sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6ce406-8ee9-4317-80eb-8526ee57d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TOTAL NUMBER OF SNOTEL SITES ##\n",
    "snotel_full = hf.get_site_variables(variable=\"swe\")\n",
    "print(len(snotel_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94946637-d41e-4c73-bb0e-41a6433c8902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NUMBER OF SITES IN TRAINING DATA ##\n",
    "run='l6_200_m40'\n",
    "test = pd.read_csv('Data/LSTM_output/'+run+'_train_metadata.csv', sep=' ')\n",
    "\n",
    "len(np.unique(test['site_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7858b485-28a5-4c9b-a3e8-72c8ecea3983",
   "metadata": {},
   "source": [
    "# Generate representative set of test sites for western US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597446c9-d89e-4826-9f13-d8e4b81c93fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## GENERATE TESTING DATA ##\n",
    "# # GET TESTING SITES\n",
    "# test = get_sites_latitude(120)\n",
    "# test = test.dropna(axis='index', how='any').reset_index().drop(columns=['index'])\n",
    "\n",
    "# test.head()\n",
    "# test.to_csv('national_test_sites.txt', sep=' ',header=None, index=False, index_label=False)\n",
    "\n",
    "# # GET TESTING YEARS\n",
    "# data_test = pd.read_csv('national_test_sites.txt', sep=' ',header=None)\n",
    "# data_test.columns = ['site_id', 'site_name', 'site_type', 'agency', 'state','first_date_data_available', 'last_date_data_available', 'record_count',\n",
    "#                      'latitude', 'longitude', 'bins','first_wy_date', 'last_wy_date']\n",
    "# #data_test = data_test.drop(columns=['bins', 'first_wy_date', 'last_wy_date'])\n",
    "# data_test\n",
    "\n",
    "# data = pd.DataFrame(columns=['site_id', 'year','train'])\n",
    "\n",
    "# for i in range(0, len(data_test)):\n",
    "#     site_id = data_test['site_id'][i]\n",
    "#     start_date = data_test['first_wy_date'][i]\n",
    "#     end_date = data_test['last_wy_date'][i]\n",
    "#     try:\n",
    "#         with redirect_stdout(trap):\n",
    "#             years = get_years_precip(start_date, end_date, site_id, 3) \n",
    "\n",
    "#         for j in range(0, len(years)):\n",
    "#             data.loc[len(data.index)] = [site_id, int(years[j]), False] \n",
    "#     except:\n",
    "#         print('missing data for ', site_id)\n",
    "#         traceback.print_exc()\n",
    "\n",
    "# data.head()\n",
    "# data.to_csv('national_test_years.txt', sep=' ',header=None, index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a258ba3-1dca-466e-b369-1bcedfbf650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENERATE NEW TESTING DATA FOR EACH MODEL ##\n",
    "RUN_name = ['l6_500']#,'l3_200','l5_200']\n",
    "#['p5_36','p10_36','p3_72','p5_72','l3_36','l5_36','l10_36','l3_72','l5_72']\n",
    "\n",
    "for run_name in RUN_name:\n",
    "    l_swe_test = [] \n",
    "    l_non_swe_test = []\n",
    "    \n",
    "    data_test = pd.read_csv('national_test_years.txt', sep=' ',header=None)\n",
    "    data_test.columns = ['site_id',\t'year',\t'train']\n",
    "    \n",
    "    # GET TESTING DATA\n",
    "    for j in range(0, len(data_test)):\n",
    "        site_id = data_test['site_id'][j]\n",
    "        year = data_test['year'][j]\n",
    "        start_date = str(year-1) + '-10-01'\n",
    "        end_date = str(year) + '-09-30'\n",
    "        try:\n",
    "            with redirect_stdout(trap):\n",
    "                swe, non_swe = get_sc_data(site_id, start_date, end_date)\n",
    "            l_swe_test.append(swe)\n",
    "            l_non_swe_test.append(non_swe)\n",
    "            # if j == 0:\n",
    "            #     plt.plot(swe['swe'],label='from hydrodata by way of code')\n",
    "    \n",
    "            # add site data\n",
    "            data_test.loc[j, 'latitude'] = non_swe.loc[0,'latitude']\n",
    "            data_test.loc[j, 'longitude'] = non_swe.loc[0,'longitude']\n",
    "            data_test.loc[j, 'elevation'] = non_swe.loc[0,'elevation']\n",
    "            data_test.loc[j, 'land cover'] = non_swe.loc[0,'land_cover']\n",
    "            data_test.loc[j, 'slope_x'] = non_swe.loc[0,'slope_x']\n",
    "            data_test.loc[j, 'slope_y'] = non_swe.loc[0,'slope_y']\n",
    "        except:\n",
    "            print('missing data for ', site_id, \" : \", year)\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    print('got data')\n",
    "    \n",
    "    test_swe = pd.concat(l_swe_test).reset_index().drop(columns='index')\n",
    "    test_non_swe = pd.concat(l_non_swe_test).reset_index().drop(columns='index')\n",
    "    #plt.plot(test_swe['swe'][0:365])\n",
    "    with open('output/'+run_name + '_normalize.pkl', 'rb') as file:  \n",
    "        l_normalize = pickle.load(file)\n",
    "    scaler_swe = l_normalize[0]\n",
    "    test_swe_tensors, test_non_swe_tensors, test_sites, test_years = create_dataset(test_swe, test_non_swe, l_normalize)\n",
    "    #plt.plot(scaler_swe.inverse_transform(test_swe_tensors[0].detach().numpy()))\n",
    "    #torch.save(test_swe_tensors, 'output/'+run_name+'_test_swe.pt')\n",
    "    #torch.save(test_non_swe_tensors, 'output/'+run_name+'_test_non_swe.pt')\n",
    "    #data_test.to_csv('output/test_metadata.csv', sep=' ',header=None, index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e1266-97e0-435b-a8cd-216105d554d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE METADATA FOR TESTING SITES AND YEARS ##\n",
    "# test_swe and test_non_swe contain all information about test data\n",
    "data_test = pd.read_csv('national_test_years.txt', sep=' ',header=None)\n",
    "data_test.columns = ['site_id',\t'year',\t'train']\n",
    "\n",
    "l_swe_test = [] \n",
    "l_non_swe_test = []\n",
    "\n",
    "# GET TESTING DATA\n",
    "count_missing = 0\n",
    "for j in tqdm(range(0, len(data_test))):\n",
    "    site_id = data_test['site_id'][j]\n",
    "    year = data_test['year'][j]\n",
    "    start_date = str(year-1) + '-10-01'\n",
    "    end_date = str(year) + '-09-30'\n",
    "    try:\n",
    "        with redirect_stdout(trap):\n",
    "            swe, non_swe = get_sc_data(site_id, start_date, end_date)\n",
    "        l_swe_test.append(swe)\n",
    "        l_non_swe_test.append(non_swe)\n",
    "\n",
    "        # add site data\n",
    "        data_test.loc[j, 'latitude'] = non_swe.loc[0,'latitude']\n",
    "        data_test.loc[j, 'longitude'] = non_swe.loc[0,'longitude']\n",
    "        data_test.loc[j, 'elevation'] = non_swe.loc[0,'elevation']\n",
    "        data_test.loc[j, 'land cover'] = non_swe.loc[0,'land_cover']\n",
    "        data_test.loc[j, 'slope_x'] = non_swe.loc[0,'slope_x']\n",
    "        data_test.loc[j, 'slope_y'] = non_swe.loc[0,'slope_y']\n",
    "    except:\n",
    "        #print('missing data for ', site_id, \" : \", year)\n",
    "        data_test.loc[j, 'missing'] = True\n",
    "        #traceback.print_exc()\n",
    "        count_missing += 1\n",
    "        \n",
    "print('number of missing training/testing sites: ',count_missing,' out of ',len(data_test),' sites')\n",
    "    \n",
    "test_swe = pd.concat(l_swe_test).reset_index().drop(columns='index')\n",
    "test_non_swe = pd.concat(l_non_swe_test).reset_index().drop(columns='index')\n",
    "test_swe.to_pickle('output/test_swe.pkl')\n",
    "test_non_swe.to_pickle('output/test_non_swe.pkl')\n",
    "\n",
    "#data_test.to_csv('output/test_metadata.csv', sep=' ',header=None, index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "925457ec-18b3-4c2d-a60a-201bdcaf2493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of shared sites for  l9_500  :  0  out of  4284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2446841/1635139469.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ev_lstm = torch.load('Data/LSTM_output/'+run+'_lstm.pt', map_location = DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics for: l9_500\n",
      "RMSE: 64.43\n",
      "normal RMSE: 0.16\n",
      "NSE: 0.57\n",
      "R2: 0.57\n",
      "Spearman's rho: 0.88\n",
      "delta peak SWE: -46.65\n",
      "normal delta peak SWE: -0.09\n",
      "absolute delta peak SWE: 95.92\n",
      "normal absolute delta peak SWE: 0.25\n",
      "delta days: 4.43\n",
      "absolute delta days: 13.58\n"
     ]
    }
   ],
   "source": [
    "## RUN OLD MODELS ON NEW TESTING DATA ## \n",
    "RUN_name = ['l9_500']\n",
    "\n",
    "# open data\n",
    "data_test = pd.read_csv('national_test_years.txt',sep=' ',header=None)\n",
    "data_test.columns = ['site_id',\t'year',\t'train']\n",
    "with open('Data/LSTM_output/test_swe.pkl', 'rb') as file:  \n",
    "    test_swe = pickle.load(file)\n",
    "with open('Data/LSTM_output/test_non_swe.pkl', 'rb') as file:  \n",
    "    test_non_swe = pickle.load(file)\n",
    "#test_swe = torch.load('/home/mcburns/national_lstm/output/'+run+'_test_swe.pt')\n",
    "#test_non_swe = torch.load('/home/mcburns/national_lstm/output/'+run+'_test_non_swe.pt')\n",
    "\n",
    "for k in range(0, len(RUN_name)):\n",
    "    ## DEFINE RUN \n",
    "    run = RUN_name[k]\n",
    "\n",
    "    # check if any sites overlap\n",
    "    metadata = pd.read_csv('Data/LSTM_output/'+run+'_train_metadata.csv',sep=' ')\n",
    "    metadata['check'] = (metadata['site_id'].isin(data_test['site_id']) & metadata['year'].isin(data_test['year']))\n",
    "    metadata[metadata['check']]\n",
    "    print(\"total number of shared sites for \", run,\" : \", metadata['check'].sum(), \" out of \", len(metadata))\n",
    "\n",
    "    ## GET DATA\n",
    "    with open('/home/mcburns/national_lstm/Data/LSTM_output/'+run+'_normalize.pkl', 'rb') as file:  \n",
    "        l_normalize = pickle.load(file)\n",
    "    scaler_swe = l_normalize[0]\n",
    "    test_swe_tensors, test_non_swe_tensors, test_sites, test_years = create_dataset(test_swe, test_non_swe, l_normalize)\n",
    "    \n",
    "    ## EVALUATE LSTM\n",
    "    ev_lstm = torch.load('Data/LSTM_output/'+run+'_lstm.pt', map_location = DEVICE)\n",
    "    \n",
    "    statistics, feature_importance = analyze_results_lstm(ev_lstm, data_test, test_swe_tensors, test_non_swe_tensors, scaler_swe, False)\n",
    "\n",
    "    ## SAVE DATA\n",
    "    feature_importance = pd.DataFrame(feature_importance)\n",
    "    feature_importance.to_csv('Data/LSTM_output/'+run+'_features.txt',sep=' ',header=None, index=False, index_label=False)\n",
    "    statistics.to_csv('Data/LSTM_output/'+run+'_statistics.txt',sep=' ',header=None, index=False, index_label=False)\n",
    "    \n",
    "    print('statistics for: ' + run)\n",
    "    print(f\"RMSE: {np.mean(statistics['rmse']):.2f}\")\n",
    "    print(f\"normal RMSE: {np.mean(statistics['normal rmse']):.2f}\")\n",
    "    print(f\"NSE: {np.mean(statistics['nse']):.2f}\")\n",
    "    print(f\"R2: {np.mean(statistics['r2']):.2f}\")\n",
    "    print(f\"Spearman's rho: {np.mean(statistics['spearman_rho']):.2f}\")\n",
    "    print(f\"delta peak SWE: {np.mean(statistics['delta peak']):.2f}\")\n",
    "    print(f\"normal delta peak SWE: {np.mean(statistics['normal delta peak']):.2f}\")\n",
    "    print(f\"absolute delta peak SWE: {np.mean(statistics['abs delta peak']):.2f}\")\n",
    "    print(f\"normal absolute delta peak SWE: {np.mean(statistics['normal abs delta peak']):.2f}\")\n",
    "    print(f\"delta days: {np.mean(statistics['delta days']):.2f}\")\n",
    "    print(f\"absolute delta days: {np.mean(statistics['abs delta days']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d178e501-fbc5-4618-ae07-a274cf557485",
   "metadata": {},
   "source": [
    "# Verify that training and testing sites do not overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba4684f-4654-4ade-a775-24b54ab3b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 'l9_500'\n",
    "\n",
    "test_sites = pd.read_csv('national_test_sites.txt',sep=' ',header=None)\n",
    "test_sites.columns=['site_id','name','type','agency','state','start date','end date','number of available sites','latitude','longitude','?','wy_start','wy_end']\n",
    "test_years = pd.read_csv('national_test_years.txt',sep=' ',header=None)\n",
    "test_years.columns=['site_id','year','train']\n",
    "\n",
    "metadata = pd.read_csv('output/'+run+'_train_metadata.csv',sep=' ')\n",
    "\n",
    "\n",
    "metadata['check'] = (metadata['site_id'].isin(test_years['site_id']) & metadata['year'].isin(test_years['year']))\n",
    "\n",
    "\n",
    "print(\"total number of shared sites: \", metadata['check'].sum(), \" out of \", len(metadata))\n",
    "\n",
    "## other checks\n",
    "# np.unique(metadata[metadata['check']]['site_id'])\n",
    "# metadata['check'] = metadata['site_id'].isin(test_sites['site_id']) \n",
    "# metadata[metadata['check']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33b1d96-cc51-445b-a269-2334cd9ad74c",
   "metadata": {},
   "source": [
    "# Number of sites in each snowpack regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d7ca8dd-2e6a-4aa5-b120-d4a78344ae72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_name</th>\n",
       "      <th>site_type</th>\n",
       "      <th>agency</th>\n",
       "      <th>state</th>\n",
       "      <th>dataset</th>\n",
       "      <th>variable</th>\n",
       "      <th>temporal_resolution</th>\n",
       "      <th>aggregation</th>\n",
       "      <th>first_date_data_available</th>\n",
       "      <th>...</th>\n",
       "      <th>record_count</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>conus1_i</th>\n",
       "      <th>conus1_j</th>\n",
       "      <th>conus2_i</th>\n",
       "      <th>conus2_j</th>\n",
       "      <th>first_wy_date</th>\n",
       "      <th>last_wy_date</th>\n",
       "      <th>num years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>301:CA:SNTL</td>\n",
       "      <td>Adin Mtn</td>\n",
       "      <td>SNOTEL station</td>\n",
       "      <td>NRCS</td>\n",
       "      <td>CA</td>\n",
       "      <td>usda_nrcs</td>\n",
       "      <td>swe</td>\n",
       "      <td>daily</td>\n",
       "      <td>sod</td>\n",
       "      <td>1984-10-01</td>\n",
       "      <td>...</td>\n",
       "      <td>14643</td>\n",
       "      <td>41.23583</td>\n",
       "      <td>-120.79192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>309.0</td>\n",
       "      <td>2086.0</td>\n",
       "      <td>1990-10-01</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>907:UT:SNTL</td>\n",
       "      <td>Agua Canyon</td>\n",
       "      <td>SNOTEL station</td>\n",
       "      <td>NRCS</td>\n",
       "      <td>UT</td>\n",
       "      <td>usda_nrcs</td>\n",
       "      <td>swe</td>\n",
       "      <td>daily</td>\n",
       "      <td>sod</td>\n",
       "      <td>1994-10-01</td>\n",
       "      <td>...</td>\n",
       "      <td>10991</td>\n",
       "      <td>37.52217</td>\n",
       "      <td>-112.27118</td>\n",
       "      <td>461.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>902.0</td>\n",
       "      <td>1525.0</td>\n",
       "      <td>2006-10-01</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>916:MT:SNTL</td>\n",
       "      <td>Albro Lake</td>\n",
       "      <td>SNOTEL station</td>\n",
       "      <td>NRCS</td>\n",
       "      <td>MT</td>\n",
       "      <td>usda_nrcs</td>\n",
       "      <td>swe</td>\n",
       "      <td>daily</td>\n",
       "      <td>sod</td>\n",
       "      <td>1996-10-01</td>\n",
       "      <td>...</td>\n",
       "      <td>10260</td>\n",
       "      <td>45.59723</td>\n",
       "      <td>-111.95902</td>\n",
       "      <td>644.0</td>\n",
       "      <td>1444.0</td>\n",
       "      <td>1090.0</td>\n",
       "      <td>2375.0</td>\n",
       "      <td>1996-10-01</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>908:WA:SNTL</td>\n",
       "      <td>Alpine Meadows</td>\n",
       "      <td>SNOTEL station</td>\n",
       "      <td>NRCS</td>\n",
       "      <td>WA</td>\n",
       "      <td>usda_nrcs</td>\n",
       "      <td>swe</td>\n",
       "      <td>daily</td>\n",
       "      <td>sod</td>\n",
       "      <td>1994-10-01</td>\n",
       "      <td>...</td>\n",
       "      <td>10991</td>\n",
       "      <td>47.77957</td>\n",
       "      <td>-121.69847</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>453.0</td>\n",
       "      <td>2778.0</td>\n",
       "      <td>1994-10-01</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>302:OR:SNTL</td>\n",
       "      <td>Aneroid Lake #2</td>\n",
       "      <td>SNOTEL station</td>\n",
       "      <td>NRCS</td>\n",
       "      <td>OR</td>\n",
       "      <td>usda_nrcs</td>\n",
       "      <td>swe</td>\n",
       "      <td>daily</td>\n",
       "      <td>sod</td>\n",
       "      <td>1982-10-01</td>\n",
       "      <td>...</td>\n",
       "      <td>15374</td>\n",
       "      <td>45.21328</td>\n",
       "      <td>-117.19258</td>\n",
       "      <td>234.0</td>\n",
       "      <td>1485.0</td>\n",
       "      <td>696.0</td>\n",
       "      <td>2421.0</td>\n",
       "      <td>1984-10-01</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>1228:UT:SNTL</td>\n",
       "      <td>Wrigley Creek</td>\n",
       "      <td>SNOTEL station</td>\n",
       "      <td>NRCS</td>\n",
       "      <td>UT</td>\n",
       "      <td>usda_nrcs</td>\n",
       "      <td>swe</td>\n",
       "      <td>daily</td>\n",
       "      <td>sod</td>\n",
       "      <td>2012-10-01</td>\n",
       "      <td>...</td>\n",
       "      <td>4414</td>\n",
       "      <td>39.13233</td>\n",
       "      <td>-111.35685</td>\n",
       "      <td>570.0</td>\n",
       "      <td>730.0</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>1682.0</td>\n",
       "      <td>2012-10-01</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>1197:UT:SNTL</td>\n",
       "      <td>Yankee Reservoir</td>\n",
       "      <td>SNOTEL station</td>\n",
       "      <td>NRCS</td>\n",
       "      <td>UT</td>\n",
       "      <td>usda_nrcs</td>\n",
       "      <td>swe</td>\n",
       "      <td>daily</td>\n",
       "      <td>sod</td>\n",
       "      <td>2012-10-01</td>\n",
       "      <td>...</td>\n",
       "      <td>4416</td>\n",
       "      <td>37.74797</td>\n",
       "      <td>-112.77495</td>\n",
       "      <td>422.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>865.0</td>\n",
       "      <td>1558.0</td>\n",
       "      <td>2012-10-01</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>878:WY:SNTL</td>\n",
       "      <td>Younts Peak</td>\n",
       "      <td>SNOTEL station</td>\n",
       "      <td>NRCS</td>\n",
       "      <td>WY</td>\n",
       "      <td>usda_nrcs</td>\n",
       "      <td>swe</td>\n",
       "      <td>daily</td>\n",
       "      <td>sod</td>\n",
       "      <td>1980-09-02</td>\n",
       "      <td>...</td>\n",
       "      <td>16133</td>\n",
       "      <td>43.93225</td>\n",
       "      <td>-109.81775</td>\n",
       "      <td>781.0</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>2171.0</td>\n",
       "      <td>1988-10-01</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>1033:CO:SNTL</td>\n",
       "      <td>Zirkel</td>\n",
       "      <td>SNOTEL station</td>\n",
       "      <td>NRCS</td>\n",
       "      <td>CO</td>\n",
       "      <td>usda_nrcs</td>\n",
       "      <td>swe</td>\n",
       "      <td>daily</td>\n",
       "      <td>sod</td>\n",
       "      <td>2002-10-01</td>\n",
       "      <td>...</td>\n",
       "      <td>8069</td>\n",
       "      <td>40.79492</td>\n",
       "      <td>-106.59544</td>\n",
       "      <td>997.0</td>\n",
       "      <td>855.0</td>\n",
       "      <td>1427.0</td>\n",
       "      <td>1801.0</td>\n",
       "      <td>2002-10-01</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>1314:OR:SNTL</td>\n",
       "      <td>Fifteenmile</td>\n",
       "      <td>SNOTEL station</td>\n",
       "      <td>NRCS</td>\n",
       "      <td>OR</td>\n",
       "      <td>usda_nrcs</td>\n",
       "      <td>swe</td>\n",
       "      <td>daily</td>\n",
       "      <td>sod</td>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>...</td>\n",
       "      <td>767</td>\n",
       "      <td>45.35265</td>\n",
       "      <td>-121.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>386.0</td>\n",
       "      <td>2525.0</td>\n",
       "      <td>2023-10-01</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>812 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          site_id         site_name       site_type agency state    dataset  \\\n",
       "0     301:CA:SNTL          Adin Mtn  SNOTEL station   NRCS    CA  usda_nrcs   \n",
       "1     907:UT:SNTL       Agua Canyon  SNOTEL station   NRCS    UT  usda_nrcs   \n",
       "2     916:MT:SNTL        Albro Lake  SNOTEL station   NRCS    MT  usda_nrcs   \n",
       "3     908:WA:SNTL    Alpine Meadows  SNOTEL station   NRCS    WA  usda_nrcs   \n",
       "4     302:OR:SNTL   Aneroid Lake #2  SNOTEL station   NRCS    OR  usda_nrcs   \n",
       "..            ...               ...             ...    ...   ...        ...   \n",
       "807  1228:UT:SNTL     Wrigley Creek  SNOTEL station   NRCS    UT  usda_nrcs   \n",
       "808  1197:UT:SNTL  Yankee Reservoir  SNOTEL station   NRCS    UT  usda_nrcs   \n",
       "809   878:WY:SNTL       Younts Peak  SNOTEL station   NRCS    WY  usda_nrcs   \n",
       "810  1033:CO:SNTL            Zirkel  SNOTEL station   NRCS    CO  usda_nrcs   \n",
       "811  1314:OR:SNTL       Fifteenmile  SNOTEL station   NRCS    OR  usda_nrcs   \n",
       "\n",
       "    variable temporal_resolution aggregation first_date_data_available  ...  \\\n",
       "0        swe               daily         sod                1984-10-01  ...   \n",
       "1        swe               daily         sod                1994-10-01  ...   \n",
       "2        swe               daily         sod                1996-10-01  ...   \n",
       "3        swe               daily         sod                1994-10-01  ...   \n",
       "4        swe               daily         sod                1982-10-01  ...   \n",
       "..       ...                 ...         ...                       ...  ...   \n",
       "807      swe               daily         sod                2012-10-01  ...   \n",
       "808      swe               daily         sod                2012-10-01  ...   \n",
       "809      swe               daily         sod                1980-09-02  ...   \n",
       "810      swe               daily         sod                2002-10-01  ...   \n",
       "811      swe               daily         sod                2022-09-28  ...   \n",
       "\n",
       "    record_count  latitude  longitude conus1_i  conus1_j  conus2_i  conus2_j  \\\n",
       "0          14643  41.23583 -120.79192      NaN       NaN     309.0    2086.0   \n",
       "1          10991  37.52217 -112.27118    461.0     569.0     902.0    1525.0   \n",
       "2          10260  45.59723 -111.95902    644.0    1444.0    1090.0    2375.0   \n",
       "3          10991  47.77957 -121.69847      NaN       NaN     453.0    2778.0   \n",
       "4          15374  45.21328 -117.19258    234.0    1485.0     696.0    2421.0   \n",
       "..           ...       ...        ...      ...       ...       ...       ...   \n",
       "807         4414  39.13233 -111.35685    570.0     730.0    1011.0    1682.0   \n",
       "808         4416  37.74797 -112.77495    422.0     602.0     865.0    1558.0   \n",
       "809        16133  43.93225 -109.81775    781.0    1234.0    1220.0    2171.0   \n",
       "810         8069  40.79492 -106.59544    997.0     855.0    1427.0    1801.0   \n",
       "811          767  45.35265    -121.53      NaN       NaN     386.0    2525.0   \n",
       "\n",
       "     first_wy_date last_wy_date num years  \n",
       "0       1990-10-01   2022-09-30        32  \n",
       "1       2006-10-01   2022-09-30        16  \n",
       "2       1996-10-01   2022-09-30        26  \n",
       "3       1994-10-01   2022-09-30        28  \n",
       "4       1984-10-01   2022-09-30        38  \n",
       "..             ...          ...       ...  \n",
       "807     2012-10-01   2022-09-30        10  \n",
       "808     2012-10-01   2022-09-30        10  \n",
       "809     1988-10-01   2022-09-30        34  \n",
       "810     2002-10-01   2022-09-30        20  \n",
       "811     2023-10-01   2022-09-30        -1  \n",
       "\n",
       "[812 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## GET DATA ##\n",
    "snotel = hf.get_site_variables(variable=\"swe\")\n",
    "snotel = snotel.reset_index(drop=True).drop(columns=['variable_name','units','site_query_url','date_metadata_last_updated','tz_cd','doi'])\n",
    "\n",
    "# read in testing data and remove it from dataset\n",
    "# data_test = pd.read_csv('national_test_sites.txt', sep=' ',header=None)\n",
    "# data_test.columns = ['site_id', 'site_name', 'site_type', 'agency', 'state','first_date_data_available', 'last_date_data_available', 'record_count',\n",
    "#                      'latitude', 'longitude', 'bins', 'first_wy_date', 'last_wy_date']\n",
    "# data_test = data_test.drop(columns=['bins'])#, 'first_wy_date', 'last_wy_date'])\n",
    "# snotel = snotel[~snotel['site_id'].isin(data_test['site_id'])].reset_index(drop=True)\n",
    "\n",
    "snotel['first_wy_date'] = snotel.apply(lambda x: get_wy_start(pd.to_datetime(x.first_date_data_available), x.site_id), axis=1)\n",
    "snotel['last_wy_date'] = snotel.apply(lambda x: get_wy_end(pd.to_datetime(x.last_date_data_available), x.site_id), axis=1)\n",
    "snotel = snotel.dropna(axis='index', subset=['first_wy_date','last_wy_date'], how='any').reset_index(drop=True)\n",
    "\n",
    "snotel['num years'] = np.array(list(int(x.split('-')[0]) for x in snotel['last_wy_date'])) - np.array(list(int(x.split('-')[0]) for x in snotel['first_wy_date']))\n",
    "#snotel = snotel[snotel['num years'] >= 3].reset_index(drop=True)\n",
    "\n",
    "snotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "217fbf44-c36e-470a-9266-b2f8b98efad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AZ', 'CA', 'CO', 'ID', 'MT', 'NM', 'NV', 'OR', 'SD', 'UT', 'WA',\n",
       "       'WY'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(snotel['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be237491-f19e-4619-9c04-c0ed3aaec684",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ? ##\n",
    "# not really sure what's going on in this code\n",
    "len(snotel[snotel['state'] == 'OR'])\n",
    "\n",
    "# bin snotel sites into 3 based on longitude\n",
    "num_sites = 200\n",
    "bins = [-125, -118, -111, -100]\n",
    "bin_labels=['maritime','intermountain','continental'] \n",
    "test = snotel\n",
    "test['bins'] = pd.cut(test['longitude'], bins=bins, labels=bin_labels, include_lowest=True)\n",
    "test\n",
    "print('intermountain: ', len(test[test['bins'] == 'intermountain']))\n",
    "print('continental: ', len(test[test['bins'] == 'continental']))\n",
    "print('maritime: ', len(test[test['bins'] == 'maritime']))\n",
    "\n",
    "pd.set_option('display.max_rows', None)    # To display all rows\n",
    "pd.set_option('display.max_columns', None) # To display all columns\n",
    "snotel[snotel['state'] == 'CA']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
